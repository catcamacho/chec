{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "from pandas import DataFrame, read_csv\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.interfaces.fsl import GLM, Split, Merge, Cluster\n",
    "from nipype.interfaces.fsl.maths import MathsCommand\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI_GZ')\n",
    "\n",
    "# Set study variables\n",
    "analysis_home = '/data/perlman/moochie/user_data/CamachoCat/ChEC/fmri_proc'\n",
    "templates_dir = '/data/perlman/moochie/user_data/CamachoCat/Aggregate_anats/templates'\n",
    "raw_dir = analysis_home + '/raw'\n",
    "preproc_dir = analysis_home + '/preproc'\n",
    "firstlevel_dir = analysis_home + '/subjectlevel'\n",
    "secondlevel_dir = analysis_home + '/grouplevelMELM'\n",
    "workflow_dir = analysis_home + '/workflows'\n",
    "affect_ratings = analysis_home + '/misc/ratings_20200427.csv'\n",
    "\n",
    "gm_mask = templates_dir + '/lcbd_template_2mm_gm.nii.gz'\n",
    "\n",
    "subject_info = read_csv(analysis_home + '/misc/subjectinfo.csv', index_col=None)\n",
    "subjects_list = subject_info['SubjID'].tolist()\n",
    "#subjects_list = ['1024','1046','1047']\n",
    "\n",
    "# data collection specs\n",
    "TR = 0.8 #in seconds\n",
    "duration= 1284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subjects list\n",
    "infosource = Node(IdentityInterface(fields=['subjid']),\n",
    "                  name='infosource')\n",
    "infosource.iterables = [('subjid', subjects_list)]\n",
    "\n",
    "# Pull files\n",
    "#file_template = {'preproc_func': preproc_dir + '/fully_processed_func/{subjid}/lomo_func.nii.gz'}\n",
    "file_template = {'preproc_func': preproc_dir + '/precensoring_func/{subjid}/resids_filt.nii.gz'}\n",
    "selectfiles = Node(SelectFiles(file_template), name='selectfiles')\n",
    "\n",
    "# Sink data of interest \n",
    "substitutions = [('_subjid_', '')] #output file name substitutions\n",
    "datasink = Node(DataSink(base_directory = firstlevel_dir,\n",
    "                        container = firstlevel_dir,\n",
    "                        substitutions = substitutions), \n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timing handling nodes\n",
    "def affectiveTiming(ratings_file,TR, duration):\n",
    "    from nipype import logging, config\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    import numpy as np\n",
    "    from pandas import read_csv\n",
    "    from os.path import abspath \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    def hrf(time):\n",
    "        from scipy.stats import gamma\n",
    "        peak = gamma.pdf(time, 6) # 6-second peak\n",
    "        undershoot = gamma.pdf(time, 12)# 12-sec undershoot\n",
    "        hrf_vals = peak-0.35*undershoot\n",
    "        return(hrf_vals)\n",
    "    \n",
    "    tr_timing = np.arange(0,duration,TR)\n",
    "    hrf_tr = hrf(tr_timing)\n",
    "    \n",
    "    ratings = read_csv(ratings_file, index_col=0)\n",
    "    regress_matrix = np.zeros((len(ratings),2))\n",
    "    regress_matrix[:,0] = np.convolve(ratings['negative'],hrf_tr)[:len(ratings)]\n",
    "    regress_matrix[:,1] = np.convolve(ratings['positive'],hrf_tr)[:len(ratings)]\n",
    "    \n",
    "    design_file_name = 'matrix.txt'\n",
    "    np.savetxt(design_file_name, regress_matrix)\n",
    "    design_file = abspath(design_file_name)\n",
    "    \n",
    "    return(design_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the design file\n",
    "make_design = Node(Function(input_names=['ratings_file', 'TR', 'duration'], \n",
    "                            output_names=['design_file'],\n",
    "                            function=affectiveTiming), name='make_design')\n",
    "make_design.inputs.TR = TR\n",
    "make_design.inputs.duration = duration\n",
    "make_design.inputs.ratings_file = affect_ratings\n",
    "\n",
    "#run the GLM\n",
    "estmodel = Node(GLM(dat_norm = True,\n",
    "                    out_file = 'betas.nii.gz', \n",
    "                    out_cope='cope.nii.gz',\n",
    "                    out_t_name = 'tstat.nii.gz', \n",
    "                    mask=gm_mask), \n",
    "                name= 'estmodel')\n",
    "\n",
    "# split the copes\n",
    "split_copes = Node(Split(dimension='t'),name='split_copes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L1workflow = Workflow(name='L1workflow_univariate')\n",
    "L1workflow.connect([(infosource,selectfiles,[('subjid','subjid')]),\n",
    "                    (selectfiles,estmodel,[('preproc_func','in_file')]),\n",
    "                    \n",
    "                    (make_design, estmodel, [('design_file','design')]),\n",
    "                    (estmodel, split_copes, [('out_cope','in_file')]),\n",
    "                    (split_copes, datasink, [('out_files','copes')]),\n",
    "                    (estmodel, datasink, [('out_t','tstats')]),\n",
    "                    (estmodel, datasink, [('out_file','betas')])\n",
    "                   ])\n",
    "L1workflow.base_dir = workflow_dir\n",
    "#L1workflow.write_graph(graph2use='flat')\n",
    "L1workflow.run('MultiProc', plugin_args={'n_procs': 6, 'memory_gb':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Effect Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melm_analysis(merged_copes,subject_info,sample_template_mask, analysis):\n",
    "    from nipype import logging, config\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    import statsmodels.formula.api as smf\n",
    "    import numpy as np\n",
    "    from pandas import DataFrame, read_csv, Series, concat\n",
    "    from nilearn.masking import apply_mask, unmask\n",
    "    from os.path import abspath\n",
    "    from warnings import filterwarnings\n",
    "\n",
    "    # Load the brain data\n",
    "    sample_data = apply_mask(merged_copes, sample_template_mask)\n",
    "    nterms = 3\n",
    "    \n",
    "    # Preallocate the output arrays\n",
    "    pvals_data = np.zeros((nterms,sample_data.shape[1])).astype(float)\n",
    "    coeffs_data = np.zeros((nterms,sample_data.shape[1])).astype(float)\n",
    "    tstat_data = np.zeros((nterms,sample_data.shape[1])).astype(float)\n",
    "\n",
    "    # define the mixed effects model\n",
    "    #formula = 'activation ~ age_cent + {0} + age_cent*{0}'.format(analysis)\n",
    "    formula = 'activation ~ age_cent'\n",
    "\n",
    "    for x in range(0,sample_data.shape[1]):\n",
    "        subject_info['activation'] = sample_data[:,x]\n",
    "        model = smf.mixedlm(formula, subject_info, groups=subject_info['male'])\n",
    "        try:\n",
    "            fitted_model = model.fit()\n",
    "            filterwarnings(\"ignore\")\n",
    "        except: \n",
    "            pass\n",
    "        else:\n",
    "            pvals_data[:,x] = np.array(fitted_model.pvalues)\n",
    "            coeffs_data[:,x] = np.array(fitted_model.params)\n",
    "            tstat_data[:,x] = np.array(fitted_model.tvalues)\n",
    "\n",
    "    #save the voxel-wise results as niftis\n",
    "    pvals_image = unmask(pvals_data, sample_template_mask)\n",
    "    pvals_image.to_filename('{0}_pvalues.nii.gz'.format(analysis))\n",
    "    coeff_image = unmask(coeffs_data, sample_template_mask)\n",
    "    coeff_image.to_filename('{0}_coefficients.nii.gz'.format(analysis))\n",
    "    tstat_image = unmask(tstat_data, sample_template_mask)\n",
    "    tstat_image.to_filename('{0}_tstats.nii.gz'.format(analysis))\n",
    "\n",
    "    pvals_file = abspath('{0}_pvalues.nii.gz'.format(analysis))\n",
    "    coeff_file = abspath('{0}_coefficients.nii.gz'.format(analysis))\n",
    "    tstat_file = abspath('{0}_tstats.nii.gz'.format(analysis))\n",
    "    \n",
    "    return(pvals_file,coeff_file,tstat_file)\n",
    "        \n",
    "def extract_cluster_betas(cluster_index_file, sample_betas, subject_ids):\n",
    "    from nipype import logging, config\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from pandas import DataFrame, Series\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    from os.path import abspath, basename\n",
    "    from nilearn.input_data import NiftiLabelsMasker\n",
    "    \n",
    "    subject_ids = sorted(subject_ids)\n",
    "    \n",
    "    ind_filename = basename(cluster_index_file) \n",
    "    out_prefix = ind_filename[:-7]\n",
    "    masker = NiftiLabelsMasker(labels_img=cluster_index_file)\n",
    "    betas = DataFrame(masker.fit_transform(sample_betas), index=subject_ids)\n",
    "    \n",
    "    betas.to_csv(out_prefix+'_extracted_values.csv')\n",
    "    extracted_betas_csv = abspath(out_prefix+'_extracted_values.csv')\n",
    "    \n",
    "    return(extracted_betas_csv)\n",
    "\n",
    "def get_cluster_peaks(clusters_file, stat_file):\n",
    "    from nipype import logging, config\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, unravel_index, max\n",
    "    from os.path import abspath\n",
    "    \n",
    "    # load up clusters\n",
    "    clusters_nii = load(clusters_file)\n",
    "    clusters_data = clusters_nii.get_data()\n",
    "    cluster_labels, cluster_sizes = unique(clusters_data, return_counts=True)\n",
    "    cluster_sizes = cluster_sizes[cluster_labels>0]\n",
    "    cluster_labels = cluster_labels[cluster_labels>0]\n",
    "    \n",
    "    # set up dataframe\n",
    "    cluster_info = DataFrame(columns=['clust_num','peak','num_voxels','X','Y','Z'])\n",
    "    cluster_info['clust_num'] = Series(cluster_labels,index=None)\n",
    "    \n",
    "    for i in range(0,len(cluster_labels)):\n",
    "        # load up stat image\n",
    "        stat_nii = load(stat_file)\n",
    "        stat_data = stat_nii.get_data()\n",
    "        stat_data[clusters_data!=cluster_labels[i]]=0\n",
    "        location=unravel_index(stat_data.argmax(), stat_data.shape)\n",
    "        cluster_info.iloc[i,0]=cluster_labels[i]\n",
    "        cluster_info.iloc[i,1]=max(stat_data)\n",
    "        cluster_info.iloc[i,2]=cluster_sizes[i]\n",
    "        cluster_info.iloc[i,3]=location[0]\n",
    "        cluster_info.iloc[i,4]=location[1]\n",
    "        cluster_info.iloc[i,5]=location[2]\n",
    "    \n",
    "    out_prefix = clusters_file[:-7]\n",
    "    cluster_info.to_csv(out_prefix + '_peaks.csv')\n",
    "    cluster_info_file = abspath(out_prefix + '_peaks.csv')\n",
    "    return(cluster_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sink data of interest \n",
    "substitutions = [('_condition_0000', 'negative'),\n",
    "                 ('_condition_0001','positive')] #output file name substitutions\n",
    "datasink = Node(DataSink(base_directory = secondlevel_dir,\n",
    "                        container = secondlevel_dir,\n",
    "                        substitutions = substitutions), \n",
    "                name='datasink')\n",
    "\n",
    "# select parameter estimates\n",
    "cope_template = {'beta_maps':firstlevel_dir + '/copes/*/vol%s.nii.gz'}\n",
    "betamap_grabber = Node(DataGrabber(sort_filelist=True,\n",
    "                                   field_template = cope_template,\n",
    "                                   base_directory=firstlevel_dir,\n",
    "                                   template=firstlevel_dir + '/copes/*/vol%s.nii.gz',\n",
    "                                   infields=['condition'],\n",
    "                                   template_args={'beta_maps':[['condition']]}), \n",
    "                       name='betamap_grabber')\n",
    "betamap_grabber.iterables=('condition',['0000','0001'])\n",
    "\n",
    "# merge individual parameter estimates together\n",
    "merge_copes = Node(Merge(dimension='t',tr=TR),name='merge_copes')\n",
    "\n",
    "# Run MELM\n",
    "melm = Node(Function(input_names=['merged_copes','subject_info','sample_template_mask','analysis'],\n",
    "                     output_names=['pvals_file','coeff_file','tstat_file'], \n",
    "                     function=melm_analysis), name='melm')\n",
    "melm.inputs.sample_template_mask = gm_mask\n",
    "melm.inputs.subject_info = subject_info\n",
    "melm.iterables=('analysis',['age'])\n",
    "#melm.iterables=('analysis',['NEG','SUR','EC'])\n",
    "\n",
    "# create invert t-stats\n",
    "inv_tstats = Node(MathsCommand(args='-mul -1',out_file='tstats_inv.nii.gz'),name='inv_tstats')\n",
    "\n",
    "# split tstats\n",
    "split_tstats = Node(Split(dimension='t'), name='split_tstats')\n",
    "\n",
    "#split inv tstats\n",
    "split_invtstats = Node(Split(dimension='t'), name='split_invtstats')\n",
    "\n",
    "# cluster results\n",
    "clust_tstats = MapNode(Cluster(threshold=3.12, dlh=2.57, out_index_file='clusters.nii.gz'), \n",
    "                       name='clust_tstats', iterfield=['in_file'])\n",
    "\n",
    "# cluster inverted results\n",
    "clust_invtstats = MapNode(Cluster(threshold=3.12, dlh=2.57, out_index_file='clusters.nii.gz'), \n",
    "                          name='clust_invtstats', iterfield=['in_file'])\n",
    "\n",
    "# extract PEs from clusters\n",
    "extract_betas = MapNode(Function(input_names=['cluster_index_file', 'sample_betas', 'subject_ids'], \n",
    "                                 output_names=['extracted_betas_csv'], \n",
    "                                 function=extract_cluster_betas), \n",
    "                        name='extract_betas', iterfield=['cluster_index_file'])\n",
    "extract_betas.inputs.subject_ids = subjects_list\n",
    "\n",
    "# extract PEs from inverted clusters\n",
    "extract_invbetas = MapNode(Function(input_names=['cluster_index_file', 'sample_betas', 'subject_ids'], \n",
    "                                    output_names=['extracted_betas_csv'], \n",
    "                                    function=extract_cluster_betas), \n",
    "                           name='extract_invbetas', iterfield=['cluster_index_file'])\n",
    "extract_invbetas.inputs.subject_ids = subjects_list\n",
    "\n",
    "# extract peaks\n",
    "extract_peaks = MapNode(Function(input_names=['clusters_file', 'stat_file'], \n",
    "                                 output_names=['cluster_info_file'], \n",
    "                                 function=get_cluster_peaks), \n",
    "                        name='extract_peaks', iterfield=['clusters_file','stat_file'])\n",
    "\n",
    "# extract inverted peaks\n",
    "extract_invpeaks = MapNode(Function(input_names=['clusters_file', 'stat_file'],\n",
    "                                    output_names=['cluster_info_file'], \n",
    "                                    function=get_cluster_peaks), \n",
    "                           name='extract_invpeaks', iterfield=['clusters_file','stat_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "melm_workflow = Workflow(name='melm_workflow_age')\n",
    "melm_workflow.connect([(betamap_grabber, merge_copes,[('beta_maps','in_files')]),\n",
    "                       (merge_copes, melm, [('merged_file','merged_copes')]),\n",
    "                       (melm, inv_tstats, [('tstat_file','in_file')]),\n",
    "                       (melm, split_tstats, [('tstat_file','in_file')]),\n",
    "                       (inv_tstats, split_invtstats, [('out_file','in_file')]),\n",
    "                       (split_tstats, clust_tstats, [('out_files','in_file')]),\n",
    "                       (split_invtstats, clust_invtstats, [('out_files','in_file')]),\n",
    "                       (merge_copes, extract_betas, [('merged_file','sample_betas')]),\n",
    "                       (merge_copes, extract_invbetas, [('merged_file','sample_betas')]),\n",
    "                       (clust_tstats, extract_betas, [('index_file','cluster_index_file')]),\n",
    "                       (clust_invtstats, extract_invbetas, [('index_file','cluster_index_file')]),\n",
    "                       (clust_tstats, extract_peaks, [('index_file','clusters_file')]),\n",
    "                       (clust_invtstats, extract_invpeaks, [('index_file','clusters_file')]),\n",
    "                       (split_tstats, extract_peaks, [('out_files','stat_file')]),\n",
    "                       (split_invtstats, extract_invpeaks, [('out_files','stat_file')]),\n",
    "                       (melm, datasink, [('pvals_file','pvals'),\n",
    "                                         ('coeff_file','coefficients'),\n",
    "                                         ('tstat_file','tstats')]),\n",
    "                       (clust_tstats, datasink, [('index_file','positive_clusters')]),\n",
    "                       (clust_invtstats, datasink, [('index_file','negative_clusters')]),\n",
    "                       (extract_betas, datasink, [('extracted_betas_csv','extracted_betas')]),\n",
    "                       (extract_invbetas, datasink, [('extracted_betas_csv','extracted_inv_betas')]),\n",
    "                       (extract_invpeaks, datasink, [('cluster_info_file','cluster_inv_tables')]),\n",
    "                       (extract_peaks, datasink, [('cluster_info_file','cluster_tables')])\n",
    "                      ])\n",
    "melm_workflow.base_dir = workflow_dir\n",
    "#melm_workflow.write_graph(graph2use='flat')\n",
    "melm_workflow.run('MultiProc', plugin_args={'n_procs': 6, 'memory_gb':32})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
